{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model and Inference Endpoint\n",
    "\n",
    "This demo shows how to build a model. You can specify quantation and lora settings, and device_map for distributing models across GPU devices.\n",
    "\n",
    "The inference endpoint show how to generate with a local model, or via API provided by OpenAI, Silicon Flow, ...\n",
    "\n",
    "This demo use transformers and vllm packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "os.environ['HF_HUB_CACHE'] = '/next_share/hf_cache/hub'\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, AutoModelForSeq2SeqLM, \n",
    "    AutoModelForSequenceClassification, AutoConfig, AutoModel, BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build with Transformers\n",
    "\n",
    "Support Lora and Quantization setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def build_gen_model(\n",
    "    model_name, \n",
    "    lora = False, \n",
    "    dtype = torch.bfloat16, \n",
    "    device_map = None,\n",
    "    quantization = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Build generation model, support quantization and lora\n",
    "    \"\"\"\n",
    "    # Determin model auto class by is_encoder_decoder\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    is_seq2seq = getattr(config, 'is_encoder_decoder', False)\n",
    "    mod_cls = AutoModelForSeq2SeqLM if is_seq2seq  else AutoModelForCausalLM\n",
    "\n",
    "    # Determin the keyword args of from_pretrained\n",
    "    ## Determin device_map. Default to the first GPU\n",
    "    if device_map is None:\n",
    "        device_map = 0\n",
    "    \n",
    "    if quantization:\n",
    "        ## Quantization config for qlora\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        quant_config = None\n",
    "\n",
    "    # prepare the initialization key words\n",
    "    kws = dict(trust_remote_code = True,\n",
    "               torch_dtype = dtype,\n",
    "               device_map = device_map,\n",
    "               quantization_config = quant_config)\n",
    "\n",
    "    # Build hf model\n",
    "    model = mod_cls.from_pretrained(model_name, **kws)\n",
    "    \n",
    "    # 3. Add lora adapter\n",
    "    if lora:\n",
    "        # determin task_type\n",
    "        task_type = \"SEQ_2_SEQ_LM\" if is_seq2seq else \"CAUSAL_LM\"\n",
    "        peft_config = LoraConfig(\n",
    "            task_type = task_type,\n",
    "            r = 16, \n",
    "            lora_alpha = 16,\n",
    "            target_modules = 'all-linear',\n",
    "            lora_dropout= 0.1,\n",
    "            bias = \"none\"\n",
    "        )\n",
    "        # Build PEFT model\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9ffc3693c1424db4c70cb565b7000e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a405f999a743493fbe14f7f49211a70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff3dc1225a14219b99bc3bd6c7f629a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "# Build a quantization model\n",
    "model = build_gen_model('gpt2', quantization = True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "x = torch.randint(0, 200, (1,100))\n",
    "out = model(input_ids = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
