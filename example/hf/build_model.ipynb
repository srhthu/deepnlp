{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "os.environ['HF_HUB_CACHE'] = '/next_share/hf_cache/hub'\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, AutoModelForSeq2SeqLM, \n",
    "    AutoModelForSequenceClassification, AutoConfig, AutoModel, BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gen_model(\n",
    "    model_name, \n",
    "    lora = False, \n",
    "    dtype = torch.bfloat16, \n",
    "    device_map = None,\n",
    "    quantization = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Build generation model, support quantization and lora\n",
    "    \"\"\"\n",
    "    # Determin model auto class by is_encoder_decoder\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    is_seq2seq = getattr(config, 'is_encoder_decoder', False)\n",
    "    mod_cls = AutoModelForSeq2SeqLM if is_seq2seq  else AutoModelForCausalLM\n",
    "\n",
    "    # Determin the keyword args of from_pretrained\n",
    "    ## Determin device_map. Default to the first GPU\n",
    "    if device_map is None:\n",
    "        device_map = 0\n",
    "    \n",
    "    ## Quantization config for qlora\n",
    "    if quantization:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        quant_config = None\n",
    "\n",
    "    kws = dict(trust_remote_code = True,\n",
    "               torch_dtype = dtype,\n",
    "               device_map = device_map,\n",
    "               quantization_config = quant_config)\n",
    "\n",
    "    # Build hf model\n",
    "    model = mod_cls.from_pretrained(model_name, **kws)\n",
    "    \n",
    "    # 3. Add lora adapter\n",
    "    if lora:\n",
    "        peft_config = LoraConfig(\n",
    "            r = 16, lora_alpha = 16,\n",
    "            target_modules = 'all-linear',\n",
    "            lora_dropout= 0.1,\n",
    "            bias = \"none\"\n",
    "        )\n",
    "        # determin task_type\n",
    "        task_type = \"SEQ_2_SEQ_LM\" if is_seq2seq else \"CAUSAL_LM\"\n",
    "        peft_config.task_type = task_type\n",
    "\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
